{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def addToDB(DB_NAME,COL_NAME,PATH,FILE):\n",
    "    '''\n",
    "    Imports a file into mongoDB\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    DB_NAME : Name of the database to connect to\n",
    "    COL_NAME: Name of the collection to create\n",
    "    PATH    : Path to folder with the file\n",
    "    FILE  : Filename\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Collection COL_NAME in DB_NAME database\n",
    "    '''\n",
    "    !mongoimport --db {DB_NAME} --collection {COL_NAME} --file {PATH+FILE} --batchSize 1\n",
    "    print(f'Collection {COL_NAME} in {DB_NAME} database created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from bson.objectid import ObjectId\n",
    "\n",
    "def show_doc(db, collection, id):\n",
    "    '''\n",
    "    Finds a document by 'id' and prints contents to the console\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    db         : database name\n",
    "    collection : mongodb collection\n",
    "    id         : mongodb document id\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Prints first 100 symbols of each document's key to console\n",
    "    '''\n",
    "    \n",
    "    doc = db[collection].find_one({'_id':ObjectId(id)})\n",
    "    for k in doc:\n",
    "        print(f\"{k} : {str(doc[k])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Some comands to keep dbs clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# deletes all 'meta' fields from all docs\n",
    "# htmlCol.update({}, {$unset: {meta:1}}, false, true); # mongo shell comand\n",
    "htmlCol.update({}, {'$unset': {'meta':1}}, multi=True) # pymongo way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# leaves only unique documents by 'url' field\n",
    "\n",
    "htmlCol.create_index(\n",
    "    \"url\",\n",
    "    unique=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pymongo 'find' returns cursor that allows iterating through results\n",
    "# calling first object [0] allows accessing the dictionary with results\n",
    "# the ['html'] is the key in the dictionary\n",
    "html = htmlCol.find({'url':'http://www.msnbc.com/velshi-ruhle/watch/jeff-sessions-is-justifying-harsh-immigration-policy-with-the-bible-1256689731629'},\\\n",
    "            projection={'html':True, '_id':False})[0]['html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find documents NOT containing a 'tag': regex expression\n",
    "import re\n",
    "tag = re.compile('dek___3AQpw.')\n",
    "docs = htmlCol.find({\"html\" : {'$not': tag}})\n",
    "for d in docs[:20]: print(d['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "# use multiprocessing to extract features\n",
    "def func():\n",
    "    DB_NAME = 'scrape'\n",
    "    db = pm.MongoClient(host='localhost', port=27017, maxPoolSize=500)[DB_NAME]\n",
    "\n",
    "    for collection in ['left','right']: docs_parser(db[collection])\n",
    "\n",
    "proc = Process(target=func)\n",
    "proc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging turn on logging to console\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# dependencies\n",
    "import pymongo as pm\n",
    "\n",
    "from resources.newspaper import newspaper\n",
    "from resources.newspaper.newspaper.source import Source\n",
    "from resources.newspaper.newspaper.article import Article\n",
    "\n",
    "import bs4 as bs\n",
    "from urllib.parse import urljoin\n",
    "from dateutil.parser import parse as date_parser\n",
    "from time import sleep\n",
    "import random\n",
    "import pytz\n",
    "\n",
    "import requests\n",
    "\n",
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pm.MongoClient(conn, maxPoolSize=200)\n",
    "\n",
    "# define db \n",
    "# DB_NAME = 'scrape'\n",
    "DB_NAME = 'FINALP'\n",
    "db = client[DB_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# powered by NewsAPI.org\n",
    "SOURCES = {\n",
    "    'left'  : [\n",
    "        'https://newrepublic.com',\n",
    "        'https://www.motherjones.com'\n",
    "# 3. Slate\n",
    "# 4. The Intercept\n",
    "# 5. Daily Beast\n",
    "# 6. The Atlantic\n",
    "# 7. Washington Post\n",
    "# 8. Politico\n",
    "# 9. The Guardian\n",
    "# 10. BBC\n",
    "    ],\n",
    "    'right' : [\n",
    "        'https://www.breitbart.com'\n",
    "# 2. Fox News\n",
    "# 3. New York Post\n",
    "# 4. The American Conservative\n",
    "# 5. Washington Times\n",
    "# 6. Daily Wire\n",
    "# 7. The Fiscal Times\n",
    "# 8. The Hill\n",
    "# 9. The Daily Caller\n",
    "# 10. Reason\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToDB(db, collection, url, html, meta={}):\n",
    "    \"\"\"\n",
    "    Saves a document to mongoDB, making sure there are no duplicates by \n",
    "    'url' value\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    db, collection  : mongo db connection\n",
    "    url, html, meta : values to store\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Saved document\n",
    "    \"\"\"\n",
    "    collection = db[collection]\n",
    "    collection.update_one(\n",
    "        {'url' : url},\n",
    "        {\n",
    "            '$set':\n",
    "                {'url' : url,\n",
    "                 'html' : html,\n",
    "                 'meta' : meta\n",
    "                }\n",
    "        }\n",
    "        ,\n",
    "        upsert=True\n",
    "    )\n",
    "    log.debug(f'Saved to DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def scrape(url, db, collection):\n",
    "    '''\n",
    "    Scrapes an article from the 'url' up to the 'latest_date'\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    url         : main news website url\n",
    "    date        : YYYY-MM-DD\n",
    "    db          : database name\n",
    "    collection  : mongodb collection\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Article's html and features stored to db, \n",
    "    Article's publish date\n",
    "    \n",
    "    '''\n",
    "    log.debug(f\"Exctracting features from {url}\")\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        # the below method may only extract a snippet... \n",
    "        # check the database for results of text extraction\n",
    "        # and apply additional processing if needed after \n",
    "        # article has been stored in the DB\n",
    "        # see code below Newrepublic for example\n",
    "        article.parse()\n",
    "    except Exception as e:\n",
    "        log.critical(f'Data not saved: {e}')\n",
    "        return datetime.datetime.now()\n",
    "    \n",
    "    saveToDB(db, collection, article.url, article.html, meta={\n",
    "        'date'    :article.publish_date,\n",
    "        'title'   :article.title,\n",
    "        'text'    :article.text,\n",
    "        'authors' :article.authors\n",
    "    })\n",
    "    \n",
    "    return article.publish_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Newrepublic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# name of collection for this media\n",
    "collection = 'newRep'\n",
    "source = 'https://newrepublic.com/latest'\n",
    "page   = 91\n",
    "\n",
    "earliest_date = date_parser('2017-01-01')\n",
    "\n",
    "while True:\n",
    "    log.debug(f'PROCESSING PAGE: {page}')\n",
    "    s = Source(source+'?page='+str(page))\n",
    "    s.download()\n",
    "\n",
    "    soup = bs.BeautifulSoup(s.html,'lxml')\n",
    "\n",
    "    # line below needs to be updated per news source\n",
    "    # to include the specific tags for article text \n",
    "    # defined differently for each site\n",
    "    for section in soup.findAll('article'):\n",
    "        url = urljoin(s.url, section.a['href'])\n",
    "        log.debug(f'Processing url: {url}')\n",
    "        \n",
    "        article_date = scrape(url, db, collection)\n",
    "\n",
    "    if article_date < earliest_date:\n",
    "        log.debug(f'Reached earliest date requested: {article_date}')\n",
    "        break\n",
    "    page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# update text field to include more data\n",
    "for doc in db[collection].find():\n",
    "    soup = bs.BeautifulSoup(doc['html'],'lxml')\n",
    "    text = ''\n",
    "    for div in soup.findAll('div',{\"class\": \"content-body\"}):\n",
    "        text += div.text\n",
    "    if len(doc['meta']['text']) < len(text):\n",
    "        db[collection].update_one(\n",
    "            {'url' : doc['url']},\n",
    "            {\n",
    "                '$set':\n",
    "                    {\n",
    "                     'meta.text' : text\n",
    "                    }\n",
    "            }\n",
    "            ,\n",
    "            upsert=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://newrepublic.com/article/139550/legacy-altamont'\n",
    "doc = db[collection].find_one({'url':url})\n",
    "doc['meta']['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "utc=pytz.UTC\n",
    "\n",
    "# name of collection for this media\n",
    "collection = 'theAtlantic'\n",
    "source = 'https://www.theatlantic.com/latest/'\n",
    "\n",
    "# start 'page' at '1' but if you run across an error\n",
    "# efficient way is to update this page to the same number\n",
    "# where you experienced the error AFTER you correct the error in the code\n",
    "# then rerun this cell\n",
    "page   = 175\n",
    "\n",
    "earliest_date = utc.localize(date_parser('2017-01-01'))\n",
    "\n",
    "while True:\n",
    "    log.debug(f'\\n\\n PROCESSING PAGE: {source+\"?page=\"+str(page)}\\n\\n\\\n",
    "              ====================================\\n\\n')\n",
    "    s = Source(source+'?page='+str(page))\n",
    "    page += 1\n",
    "    s.download()\n",
    "    soup = bs.BeautifulSoup(s.html,'lxml')\n",
    "\n",
    "    # line below needs to be updated per news source\n",
    "    # to include the specific tags for article text \n",
    "    # defined differently for each site\n",
    "    for section in soup.findAll('li', {\"class\":\"article\"}):\n",
    "        url = urljoin(s.url, section.a['href'])\n",
    "        log.debug(f'Processing url: {url}')\n",
    "        \n",
    "        try:\n",
    "            article_date = scrape(url, db, collection)\n",
    "        except Exception:\n",
    "            article_date = earliest_date + 1 #to make sure scraping continues\n",
    "        \n",
    "        # the Atlantic blocks right away after few quick downloads\n",
    "        # so it requires sleeping, testing showed 1 to 5 seconds is enough\n",
    "#         sleep(random.uniform(1,5))\n",
    "\n",
    "    try:\n",
    "        if article_date < earliest_date:\n",
    "            log.debug(f'Reached earliest date requested: {article_date}')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        log.debug(f'Exception: {e}')\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Politico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "# use your newsapi key\n",
    "# update collection, date and source\n",
    "collection = 'Politoco'\n",
    "page = 1\n",
    "earliest_date = '2017-01-01'\n",
    "latest_date = '2017-09-13'\n",
    "source = 'politico'\n",
    "pageSize = 100\n",
    "apiKey = 'fd1386b678fd4524b2aa84e3bee1f8f7'\n",
    "\n",
    "params = {\n",
    "    'apiKey' : apiKey,\n",
    "    'pageSize' : pageSize,\n",
    "    'page' : page,\n",
    "    'from' : earliest_date,\n",
    "    'to'   : latest_date,\n",
    "    'sources': source\n",
    "}\n",
    "\n",
    "# base url\n",
    "api_url = 'https://newsapi.org/v2/everything?'\n",
    "\n",
    "# get first request to obtain total news count\n",
    "r = requests.get(api_url, params=params)\n",
    "totalPages = r.json()['totalResults']//100\n",
    "log.debug(f'Total pages ({pageSize} per page) for {source} results from {earliest_date} to {latest_date} is {totalPages}')\n",
    "\n",
    "# scrape news\n",
    "for p in range(page,totalPages+1):\n",
    "    log.debug(f'\\n\\n PROCESSING PAGE: {source+\"?page=\"+str(page)}\\n\\n\\\n",
    "              ====================================\\n\\n')\n",
    "    \n",
    "    params = {\n",
    "        'apiKey' : apiKey,\n",
    "        'pageSize' : pageSize,\n",
    "        'page' : page,\n",
    "        'from' : earliest_date,\n",
    "        'to'   : latest_date,\n",
    "        'sources': source\n",
    "    }\n",
    "    \n",
    "    r = requests.get(api_url, params=params)\n",
    "\n",
    "    page += 1\n",
    "    \n",
    "    if r.json()['articles']:\n",
    "        for a in r.json()['articles']:\n",
    "            try:\n",
    "                log.debug(f\"Processing url: {a['url']}\")\n",
    "                article_date = scrape(a['url'], db, collection)\n",
    "            except Exception:\n",
    "                article_date = earliest_date + 1\n",
    "    else:\n",
    "        log.debug(f\"Response doesn't have articles. Response: {r.json()}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r.json()['totalResults']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Washington Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "# use your newsapi key\n",
    "# update collection, date and source\n",
    "collection = 'WashTimes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "page = 1\n",
    "earliest_date = '2017-01-01'\n",
    "latest_date = '2017-12-31'\n",
    "source = 'the-washington-times'\n",
    "pageSize = 100\n",
    "apiKey = 'fd1386b678fd4524b2aa84e3bee1f8f7'\n",
    "\n",
    "params = {\n",
    "    'apiKey'   : apiKey,\n",
    "    'pageSize' : pageSize,\n",
    "    'page'     : page,\n",
    "    'from'     : earliest_date,\n",
    "    'to'       : latest_date,\n",
    "    'sources'  : source\n",
    "}\n",
    "\n",
    "# base url\n",
    "api_url = 'https://newsapi.org/v2/everything?'\n",
    "\n",
    "# get first request to obtain total news count\n",
    "r = requests.get(api_url, params=params)\n",
    "totalPages = r.json()['totalResults']//100\n",
    "\n",
    "log.debug(f'TOTAL PAGES FOR {source}: {totalPages}')\n",
    "\n",
    "# scrape news\n",
    "for p in range(page,totalPages):\n",
    "    log.debug(f'\\n\\n PROCESSING PAGE: {source+\"?page=\"+str(page)}\\n\\n\\\n",
    "              ====================================\\n\\n')\n",
    "    \n",
    "    params = {\n",
    "        'apiKey'   : apiKey,\n",
    "        'pageSize' : pageSize,\n",
    "        'page'     : page,\n",
    "        'from'     : earliest_date,\n",
    "        'to'       : latest_date,\n",
    "        'sources'  : source\n",
    "    }\n",
    "    \n",
    "    r = requests.get(api_url, params=params)\n",
    "\n",
    "    page += 1\n",
    "    \n",
    "    for a in r.json()['articles']:\n",
    "        try:\n",
    "            log.debug(f\"Processing url: {a['url']}\")\n",
    "            if db[collection].find({'url':a['url']}):\n",
    "                log.debug(f'URL exists in DB. Skipping.')\n",
    "            else:\n",
    "                article_date = scrape(a['url'], db, collection)\n",
    "        except Exception:\n",
    "            article_date = earliest_date + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "collection = 'slate'\n",
    "source = 'https://slate.com/news-and-politics/'\n",
    "page   = 118\n",
    "\n",
    "utc=pytz.UTC\n",
    "earliest_date = utc.localize(date_parser('2017-01-01'))\n",
    "\n",
    "while True:\n",
    "    log.debug(f'\\n\\n PROCESSING PAGE: {source+str(page)}\\n\\n\\\n",
    "              ====================================\\n\\n')\n",
    "    s = Source(source+str(page))\n",
    "    page += 1\n",
    "    s.download()\n",
    "    soup = bs.BeautifulSoup(s.html,'lxml')\n",
    "\n",
    "    # line below needs to be updated per news source\n",
    "    # to include the specific tags for article text \n",
    "    # defined differently for each site\n",
    "    for link in soup.find('div', {\"class\":\"topic-stories-list\"}).findChildren(\"a\" , recursive=False):\n",
    "        url = link['href']\n",
    "        log.debug(f'Processing url: {url}')\n",
    "        \n",
    "        try:\n",
    "            article_date = scrape(url, db, collection)\n",
    "        except Exception:\n",
    "            article_date = earliest_date + 1 #to make sure scraping continues\n",
    "        \n",
    "        # the Atlantic blocks right away after few quick downloads\n",
    "        # so it requires sleeping, testing showed 1 to 5 seconds is enough\n",
    "#         sleep(random.uniform(1,5))\n",
    "\n",
    "    try:\n",
    "        if article_date < earliest_date:\n",
    "            log.debug(f'Reached earliest date requested: {article_date}')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        log.debug(f'Exception: {e}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "collection = 'theintercept'\n",
    "source = 'https://theintercept.com'\n",
    "\n",
    "# for infinite scroll page\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import sys\n",
    "\n",
    "import unittest, time, re\n",
    "\n",
    "logging.getLogger('selenium').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Sel(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.implicitly_wait(30)\n",
    "        self.base_url = source\n",
    "        self.verificationErrors = []\n",
    "        self.accept_next_alert = True\n",
    "    def getPage(self):\n",
    "        driver = self.driver\n",
    "        delay = 2\n",
    "        driver.get(self.base_url)\n",
    "        html_source = driver.page_source\n",
    "        self.html = html_source.encode('utf-8')\n",
    "        return self.html\n",
    "    def scrollDown(self):\n",
    "        driver = self.driver\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        self.html = driver.page_source.encode('utf-8')\n",
    "        return driver.page_source \n",
    "    def shutdown(self):\n",
    "        driver = self.driver\n",
    "        driver.quit()\n",
    "\n",
    "data = Sel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.setUp()\n",
    "data.getPage()\n",
    "page = 1\n",
    "\n",
    "utc=pytz.UTC\n",
    "earliest_date = utc.localize(date_parser('2017-01-01'))\n",
    "\n",
    "scraped_urls = []\n",
    "\n",
    "while True:\n",
    "    log.debug(f'NEXT SCROLL (#{page})')\n",
    "    page += 1\n",
    "    \n",
    "    html = data.scrollDown()\n",
    "    soup = bs.BeautifulSoup(html,'lxml')\n",
    "\n",
    "    for link in soup.find('div', {\"id\":\"recently\"}).findAll('a'):\n",
    "        url = urljoin(source, link['href'])\n",
    "        if url and url not in scraped_urls:\n",
    "            scraped_urls.append(url)\n",
    "            log.debug(f'Processing url: {url}')\n",
    "            article_date = scrape(url, db, collection)\n",
    "\n",
    "    try:\n",
    "        if article_date < earliest_date:\n",
    "            log.debug(f'Reached earliest date requested: {article_date}')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        log.debug(f\"Something is wrong: {e}\")\n",
    "data.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The daily beast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.thedailybeast.com\n",
      "DEBUG:urllib3.connectionpool:https://www.thedailybeast.com:443 \"GET /sitemap HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "collection = 'thedailybeast'\n",
    "source = 'https://www.thedailybeast.com/sitemap'\n",
    "earliest_date = date_parser('2017-01-01')\n",
    "s = Source(source)\n",
    "s.download()\n",
    "soup = bs.BeautifulSoup(s.html,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['https://www.thedailybeast.com/sitemap/2018/1/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/2/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/3/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/4/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/5/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/6/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/7/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/1/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/2/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/3/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/4/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/5/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/6/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/7/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/8/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/9/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/10/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/11/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/12/article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in months:\n",
    "    log.debug(f\"\\n=======PROCESSING PAGE: {month}\\n\")\n",
    "    s = Source(month)\n",
    "    s.download()\n",
    "    soup = bs.BeautifulSoup(s.html,'lxml')\n",
    "    for a in soup.find('div',{'class':'SitemapMonthPage__articles-wrapper'}).findAll('a'):\n",
    "        url = urljoin(source, a['href'])\n",
    "        log.debug(f'Processing url: {url}')\n",
    "        \n",
    "        article_date = scrape(url, db, collection) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): content.guardianapis.com\n",
      "DEBUG:urllib3.connectionpool:https://content.guardianapis.com:443 \"GET /search?api-key=53aee7b4-8c94-495b-b13b-ea7214ba0ca2&edition=us&section=us-news&from-date=2017-01-01&to-date=2018-02-13&page-size=200&page=1 HTTP/1.1\" 200 16468\n",
      "DEBUG:__main__:\n",
      "================================\n",
      "PROCESSING PAGE: 1\n",
      "\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): content.guardianapis.com\n",
      "DEBUG:urllib3.connectionpool:https://content.guardianapis.com:443 \"GET /search?api-key=53aee7b4-8c94-495b-b13b-ea7214ba0ca2&edition=us&section=us-news&from-date=2017-01-01&to-date=2018-02-13&page-size=200&page=1 HTTP/1.1\" 200 None\n",
      "DEBUG:__main__:Processing apiUrl: https://content.guardianapis.com/us-news/2018/feb/13/trump-rob-porter-background-check-wife-fbi-chris-wray-contradiction\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): content.guardianapis.com\n",
      "DEBUG:urllib3.connectionpool:https://content.guardianapis.com:443 \"GET /us-news/2018/feb/13/trump-rob-porter-background-check-wife-fbi-chris-wray-contradiction?api-key=53aee7b4-8c94-495b-b13b-ea7214ba0ca2&show-fields=all HTTP/1.1\" 200 3767\n",
      "DEBUG:__main__:Weburl: https://www.theguardian.com/us-news/2018/feb/13/trump-rob-porter-background-check-wife-fbi-chris-wray-contradiction\n",
      "DEBUG:__main__:Saved to DB\n",
      "DEBUG:__main__:Processing apiUrl: https://content.guardianapis.com/us-news/2018/feb/13/trump-budget-cuts-safety-net-for-poorest-americans\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): content.guardianapis.com\n",
      "DEBUG:urllib3.connectionpool:https://content.guardianapis.com:443 \"GET /us-news/2018/feb/13/trump-budget-cuts-safety-net-for-poorest-americans?api-key=53aee7b4-8c94-495b-b13b-ea7214ba0ca2&show-fields=all HTTP/1.1\" 200 5001\n",
      "DEBUG:__main__:Weburl: https://www.theguardian.com/us-news/2018/feb/13/trump-budget-cuts-safety-net-for-poorest-americans\n",
      "DEBUG:__main__:Saved to DB\n",
      "DEBUG:__main__:Processing apiUrl: https://content.guardianapis.com/us-news/2018/feb/13/remington-bankruptcy-guns-trump-slump-sales\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): content.guardianapis.com\n",
      "DEBUG:urllib3.connectionpool:https://content.guardianapis.com:443 \"GET /us-news/2018/feb/13/remington-bankruptcy-guns-trump-slump-sales?api-key=53aee7b4-8c94-495b-b13b-ea7214ba0ca2&show-fields=all HTTP/1.1\" 200 4056\n",
      "DEBUG:__main__:Weburl: https://www.theguardian.com/us-news/2018/feb/13/remington-bankruptcy-guns-trump-slump-sales\n",
      "DEBUG:__main__:Saved to DB\n",
      "DEBUG:__main__:Processing apiUrl: https://content.guardianapis.com/us-news/2018/feb/13/meet-the-sacklers-the-family-feuding-over-blame-for-the-opioid-crisis\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): content.guardianapis.com\n",
      "DEBUG:urllib3.connectionpool:https://content.guardianapis.com:443 \"GET /us-news/2018/feb/13/meet-the-sacklers-the-family-feuding-over-blame-for-the-opioid-crisis?api-key=53aee7b4-8c94-495b-b13b-ea7214ba0ca2&show-fields=all HTTP/1.1\" 200 10539\n",
      "DEBUG:__main__:Weburl: https://www.theguardian.com/us-news/2018/feb/13/meet-the-sacklers-the-family-feuding-over-blame-for-the-opioid-crisis\n",
      "DEBUG:__main__:Saved to DB\n",
      "DEBUG:__main__:Processing apiUrl: https://content.guardianapis.com/us-news/2018/feb/13/us-military-guns-background-checks-texas-shooting\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): content.guardianapis.com\n",
      "DEBUG:urllib3.connectionpool:https://content.guardianapis.com:443 \"GET /us-news/2018/feb/13/us-military-guns-background-checks-texas-shooting?api-key=53aee7b4-8c94-495b-b13b-ea7214ba0ca2&show-fields=all HTTP/1.1\" 200 3337\n",
      "DEBUG:__main__:Weburl: https://www.theguardian.com/us-news/2018/feb/13/us-military-guns-background-checks-texas-shooting\n",
      "DEBUG:__main__:Saved to DB\n",
      "DEBUG:__main__:Processing apiUrl: https://content.guardianapis.com/us-news/2018/feb/13/trump-infrastructure-plan-environmental-reviews\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): content.guardianapis.com\n",
      "DEBUG:urllib3.connectionpool:https://content.guardianapis.com:443 \"GET /us-news/2018/feb/13/trump-infrastructure-plan-environmental-reviews?api-key=53aee7b4-8c94-495b-b13b-ea7214ba0ca2&show-fields=all HTTP/1.1\" 200 2899\n",
      "DEBUG:__main__:Weburl: https://www.theguardian.com/us-news/2018/feb/13/trump-infrastructure-plan-environmental-reviews\n",
      "DEBUG:__main__:Saved to DB\n",
      "DEBUG:__main__:Processing apiUrl: https://content.guardianapis.com/us-news/2018/feb/12/eye-worms-oregon-woman\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): content.guardianapis.com\n",
      "DEBUG:urllib3.connectionpool:https://content.guardianapis.com:443 \"GET /us-news/2018/feb/12/eye-worms-oregon-woman?api-key=53aee7b4-8c94-495b-b13b-ea7214ba0ca2&show-fields=all HTTP/1.1\" 200 1763\n",
      "DEBUG:__main__:Weburl: https://www.theguardian.com/us-news/2018/feb/12/eye-worms-oregon-woman\n",
      "DEBUG:__main__:Saved to DB\n",
      "DEBUG:__main__:Processing apiUrl: https://content.guardianapis.com/world/gallery/2012/may/31/us-presidential-portraits-pictures\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): content.guardianapis.com\n"
     ]
    }
   ],
   "source": [
    "collection    = 'theguardian'\n",
    "source        = 'https://content.guardianapis.com/search'\n",
    "apikey        = '53aee7b4-8c94-495b-b13b-ea7214ba0ca2'\n",
    "earliest_date = '2017-01-01'\n",
    "latest_date   = '2018-02-13'\n",
    "section       = 'us-news'\n",
    "edition       = 'us'\n",
    "page          = 1\n",
    "page_size     = 200\n",
    "\n",
    "params = {\n",
    "    'api-key'   : apikey,\n",
    "    'edition'   : edition,\n",
    "    'section'   : section,\n",
    "    'from-date' : earliest_date,\n",
    "    'to-date'   : latest_date,\n",
    "    'page-size' : page_size,\n",
    "    'page'      : page\n",
    "}\n",
    "\n",
    "r = requests.get(source, params=params)\n",
    "\n",
    "response = r.json()['response']\n",
    "pages    = response['pages']\n",
    "\n",
    "for page in range(1,pages):\n",
    "    log.debug(f\"\\n================================\\nPROCESSING PAGE: {page}\\n\")\n",
    "    \n",
    "    params = {\n",
    "    'api-key'   : apikey,\n",
    "    'edition'   : edition,\n",
    "    'section'   : section,\n",
    "    'from-date' : earliest_date,\n",
    "    'to-date'   : latest_date,\n",
    "    'page-size' : page_size,\n",
    "    'page'      : page\n",
    "    }\n",
    "    \n",
    "    r = requests.get(source, params=params)\n",
    "    response = r.json()['response']\n",
    "    \n",
    "    for article in response['results']:\n",
    "        apiUrl = article['apiUrl']\n",
    "        log.debug(f'Processing apiUrl: {apiUrl}')\n",
    "        a = requests.get(apiUrl, params={'api-key':apikey,'show-fields': 'all'}).json()['response']['content']\n",
    "\n",
    "        html  = a['fields']['main'] + a['fields']['body']\n",
    "        url   = a['webUrl']\n",
    "        log.debug(f'Weburl: {url}')\n",
    "        \n",
    "        try:\n",
    "            saveToDB(db, collection, url, html, meta={\n",
    "                'date'    : date_parser(a['fields']['firstPublicationDate']),\n",
    "                'title'   : a['webTitle'],\n",
    "                'text'    : a['fields']['bodyText'],\n",
    "                'apiURL'  : url\n",
    "            })  \n",
    "        except Exception as e:\n",
    "            log.debug(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://content.guardianapis.com/world/gallery/2012/may/31/us-presidential-portraits-pictures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
